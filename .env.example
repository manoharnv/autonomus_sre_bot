# AI Model Configuration
MODEL=gpt-4o-mini
OPENAI_API_KEY=<deepseek-api-key>
OPENAI_API_BASE=https://api.deepseek.com
OPENAI_MODEL_NAME=deepseek/deepseek-chat

# Middleware.io Integration
MIDDLEWARE_API_KEY=<middleware-api-key>

# Gemini API Configuration
GEMINI_API_KEY=<gemini-api-key>
GEMINI_EMBEDDER_MODEL_NAME=gemini-embedding-001

# GitHub Integration (for MCP and self-healing crew)
GITHUB_TOKEN=<github-personal-access-token>
GITHUB_OWNER=<github-username-or-org>
GITHUB_REPO=<repository-url-or-name>

# Atlassian Integration (JSM and MCP)
ATLASSIAN_CLOUD_ID=<your-atlassian-cloud-id>
ATLASSIAN_TOKEN=<your-atlassian-access-token>
ATLASSIAN_SERVICE_DESK_ID=<your-service-desk-id>
ATLASSIAN_REQUEST_TYPE_ID=<your-request-type-id>
ATLASSIAN_INCIDENT_REQUEST_TYPE_ID=<your-incident-request-type-id>
ATLASSIAN_USER_ID=<your-atlassian-user-id>
ATLASSIAN_URL=https://yoursite.atlassian.net

# Kubernetes Integration (for self-healing crew)
KUBECONFIG=~/.kube/config
KUBERNETES_NAMESPACE=production

# Self-Healing Crew Configuration
SELF_HEAL_POLLING_INTERVAL=300
SELF_HEAL_MAX_RETRIES=3
SELF_HEAL_TIMEOUT_MINUTES=30

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE_PATH=logs/autonomous_sre.log

# MCP Atlassian Server Configuration
TRANSPORT=streamable-http
PORT=8080
HOST=0.0.0.0
MCP_VERBOSE=true
MCP_VERY_VERBOSE=false
MCP_LOGGING_STDOUT=true
READ_ONLY_MODE=false
CONFLUENCE_SPACES_FILTER=
JIRA_PROJECTS_FILTER=
ENABLED_TOOLS=

# Langfuse Observability
LANGFUSE_PUBLIC_KEY=<langfuse-public-key>
LANGFUSE_SECRET_KEY=<langfuse-secret-key>
LANGFUSE_HOST=https://cloud.langfuse.com
OTEL_EXPORTER_OTLP_ENDPOINT=https://cloud.langfuse.com/api/public/otel